activation = "relu",
input_shape = ncol(X_trn)) %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 16, activation = "relu") %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 1)
net <- compile(net, loss = "mse",
optimizer = "adam",
metrics = "mse")    # alternativa: "mae"
install.packages("tensorflow")
install.packages("tensorflow")
library(keras)
install_keras()
library("reticulate")
# 2. No RStudio, instale a biblioteca devtools:
install.packages("devtools")
install.packages("devtools")
# 3. No RStudio, instale a biblioteca Keras:
library(devtools)
devtools::install_github("rstudio/keras")
library(keras)
install_keras()
library(keras)
mnist <- dataset_mnist()
library(keras)
library(devtools)
library(MASS)
library(tidyverse)
library(rsample)
library(yardstick)
library(tensorflow)
set.seed(1234)
split <- initial_split(Boston, prop = 0.7)
training <- training(split)
test <- testing(split)
View(training)
View(test)
# Preparando os dados para a rede neural ----------------------------------
# se tiver variavel categorica, transformar em dummy
X_trn <- training %>%
select(-medv) %>%
as.matrix()
View(X_trn)
X_tst <- test %>%
select(-medv) %>%
as.matrix()
X_trn <- scale(X_trn)
view(X_trn)
X_tst <- scale(X_tst,
center = attr(X_trn, "scaled:center"),
scale = attr(X_trn, "scaled:scale"))
net <- keras_model_sequential() %>%
layer_dense(units = 32,
activation = "relu",
input_shape = ncol(X_trn)) %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 16, activation = "relu") %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 1)
devtools::install_github("rstudio/keras")
library(keras)
install_keras()
library(keras)
mnist <- dataset_mnist()
install.packages("tensorflow")
install.packages("tensorflow")
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
library(keras)
library(devtools)
library(MASS)
library(tidyverse)
library(rsample)
library(yardstick)
library(tensorflow)
# 2. No RStudio, instale a biblioteca devtools:
install.packages("devtools")
install.packages("devtools")
# 3. No RStudio, instale a biblioteca Keras:
library(devtools)
devtools::install_github("rstudio/keras")
library(keras)
install_keras()
library(keras)
mnist <- dataset_mnist()
install.packages("tensorflow")
install.packages("tensorflow")
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
library(keras)
library(devtools)
library(MASS)
library(tidyverse)
library(rsample)
library(yardstick)
library(tensorflow)
conflicted::conflict_prefer("select", "dplyr")
set.seed(1234)
split <- initial_split(Boston, prop = 0.7)
training <- training(split)
test <- testing(split)
View(training)
View(test)
# Preparando os dados para a rede neural ----------------------------------
# se tiver variavel categorica, transformar em dummy
X_trn <- training %>%
select(-medv) %>%
as.matrix()
View(X_trn)
X_tst <- test %>%
select(-medv) %>%
as.matrix()
X_trn <- scale(X_trn)
view(X_trn)
X_tst <- scale(X_tst,
center = attr(X_trn, "scaled:center"),
scale = attr(X_trn, "scaled:scale"))
net <- keras_model_sequential() %>%
layer_dense(units = 32,
activation = "relu",
input_shape = ncol(X_trn)) %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 16, activation = "relu") %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 1)
library("reticulate")
reticulate::use_python("C:/Users/helio/AppData/Local/Programs/Python/Python312python.exe")
reticulate::use_python("C://Users//helio//AppData//Local//Programs//Python//Python312//python.exe")
install.packages("tensorflow")
install.packages("tensorflow")
library(keras)
library(devtools)
library(MASS)
library(tidyverse)
library(rsample)
library(yardstick)
library(tensorflow)
set.seed(1234)
split <- initial_split(Boston, prop = 0.7)
training <- training(split)
test <- testing(split)
View(training)
View(test)
# Preparando os dados para a rede neural ----------------------------------
# se tiver variavel categorica, transformar em dummy
X_trn <- training %>%
select(-medv) %>%
as.matrix()
View(X_trn)
X_tst <- test %>%
select(-medv) %>%
as.matrix()
X_trn <- scale(X_trn)
view(X_trn)
X_tst <- scale(X_tst,
center = attr(X_trn, "scaled:center"),
scale = attr(X_trn, "scaled:scale"))
net <- keras_model_sequential() %>%
layer_dense(units = 32,
activation = "relu",
input_shape = ncol(X_trn)) %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 16, activation = "relu") %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 1)
reticulate::use_python("C:/Users/helio/anaconda3/envs/r-tensorflow/python.exe")
library("reticulate")
reticulate::use_python("C:/Users/helio/anaconda3/envs/r-tensorflow/python.exe")
library(keras)
install_keras()
# 2. No RStudio, instale a biblioteca devtools:
install.packages("devtools")
install.packages("devtools")
# 3. No RStudio, instale a biblioteca Keras:
library(devtools)
devtools::install_github("rstudio/keras")
# 4. Carregue a biblioteca Keras e finalize a instalação.
library(keras)
install_keras()
# 2. No RStudio, instale a biblioteca devtools:
install.packages("devtools")
install.packages("devtools")
# 4. Carregue a biblioteca Keras e finalize a instalação.
library(keras)
install_keras()
library(keras)
mnist <- dataset_mnist()
library(keras)
mnist <- dataset_mnist()
pip install --upgrade pip
pip install
pip install --upgrade pip
# 2. No RStudio, instale a biblioteca devtools:
install.packages("devtools")
install.packages("devtools")
# 3. No RStudio, instale a biblioteca Keras:
library(devtools)
devtools::install_github("rstudio/keras")
# 4. Carregue a biblioteca Keras e finalize a instalação.
library(keras)
install_keras()
# 4. Carregue a biblioteca Keras e finalize a instalação.
library(keras)
install_keras()
# 3. No RStudio, instale a biblioteca Keras:
library(devtools)
devtools::install_github("rstudio/keras")
# 4. Carregue a biblioteca Keras e finalize a instalação.
library(keras)
install_keras()
install.packages(“reticulate”)
install.packages("reticulate")
install.packages("reticulate")
library(keras)
mnist <- dataset_mnist()
library(keras)
library(devtools)
library(MASS)
library(tidyverse)
library(rsample)
library(yardstick)
library(tensorflow)
set.seed(1234)
split <- initial_split(Boston, prop = 0.7)
training <- training(split)
test <- testing(split)
View(training)
View(test)
# Preparando os dados para a rede neural ----------------------------------
# se tiver variavel categorica, transformar em dummy
X_trn <- training %>%
select(-medv) %>%
as.matrix()
View(X_trn)
X_tst <- test %>%
select(-medv) %>%
as.matrix()
X_trn <- scale(X_trn)
view(X_trn)
X_tst <- scale(X_tst,
center = attr(X_trn, "scaled:center"),
scale = attr(X_trn, "scaled:scale"))
net <- keras_model_sequential() %>%
layer_dense(units = 32,
activation = "relu",
input_shape = ncol(X_trn)) %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 16, activation = "relu") %>%
# layer_dropout(rate = 0.1) %>%
layer_dense(units = 8, activation = "relu") %>%
layer_dense(units = 1)
net <- compile(net, loss = "mse",
optimizer = "adam",
metrics = "mse")    # alternativa: "mae"
summary(net)
history <- fit(net, X_trn, training$medv,
batch_size = 16, epochs = 20, # defaults: 32, 10
validation_split = 0.2)
y_hat_net <- predict(net, X_tst)
RMSE_net <- rmse_vec(test$medv, as.numeric(y_hat_net))
(RMSE_net <- sqrt(mean((y_hat_net - test$medv)^2)))
linear <- lm(medv ~ ., training)
y_hat_lm <- predict(linear, test)
(RMSE_lm <- sqrt(mean((y_hat_lm - test$medv)^2)))
# Floresta aleatória ------------------------------------------------------
library(randomForest)
rf <- randomForest(medv ~ ., training)
y_hat_rf <- predict(rf, test)
(RMSE_rf <- sqrt(mean((y_hat_rf - test$medv)^2)))
RMSE_net
RMSE_lm
RMSE_rf
setwd("C:/Users/helio/OneDrive/Documentos/Learning/INSPER/2Tri23/Aprendizagem Estatística II/Trabalho_Final/TrabalhoFinalAEM2")
library(MASS)
library(tidyverse)
library(keras)
library(rsample)
library(keras)
library(yardstick)
library(skimr)
library(dplyr)
library(GGally)
library(ggrepel)
library(factoextra)
library(countrycode)
library(dplyr)
df <- read.csv("https://raw.githubusercontent.com/mconjaud/TrabalhoFinalAEM2/main/coffee_ratings.csv")
# Calculando a proporção de valores missing por variável
prop_missing <- df %>%
summarise_all(~ mean(is.na(.))) %>%
gather() %>%
arrange(desc(value))
grafico <- ggplot(prop_missing, aes(x = reorder(key, -value), y = value)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +  # Rotaciona as barras para torná-las horizontais
labs(x = "Variável", y = "% de Valores missing",
title = "% de Valores missing por Variável")
print(grafico)
library(car)
# Calculando a proporção de valores missing por variável
vif <- vif(lm(df$total_cup_points ~., data = df))
# Calculando a proporção de valores missing por variável
vif <- vif(lm(total_cup_points ~., data = df))
df <- read.csv("https://raw.githubusercontent.com/mconjaud/TrabalhoFinalAEM2/main/coffee_ratings.csv")
#### ANALIZANDO A BASE ############
View(df) #Vizualizando a base
str(df)  # Tipo da variavel
skim(df) # % de missings
# Calculando a proporção de valores missing por variável
prop_missing <- df %>%
summarise_all(~ mean(is.na(.))) %>%
gather() %>%
arrange(desc(value))
grafico <- ggplot(prop_missing, aes(x = reorder(key, -value), y = value)) +
geom_bar(stat = "identity", fill = "skyblue") +
coord_flip() +  # Rotaciona as barras para torná-las horizontais
labs(x = "Variável", y = "% de Valores missing",
title = "% de Valores missing por Variável")
print(grafico)
#### TRATANDO A BASE ANTES DE APLICAR MODELOS ############
# Removendo as variáveis que não serão utilizadas
coffee <- subset (df,
select = -c(owner
,farm_name
,lot_number
,mill
,ico_number
,company
,altitude
,region
,producer
,number_of_bags
,bag_weight
,in_country_partner
,harvest_year
,grading_date
,owner_1
#,variety
,expiration
,certification_body
,certification_address
,certification_contact
,altitude_low_meters
,altitude_high_meters))
# Trocar missing por "Washed / Wet"
unique(coffee$processing_method)
coffee <- coffee %>%
mutate(processing_method =
ifelse(is.na(processing_method), "Washed / Wet", processing_method))
# Trocar missing por "Green"
unique(coffee$color)
coffee <- coffee %>%
mutate(color = ifelse(is.na(color), "Green", color))
# Padronizar "unit_of_measurement" em metros
unique(coffee$unit_of_measurement)
view(coffee$unit_of_measurement)
coffee <- coffee %>%
mutate(unit_of_measurement =
ifelse(unit_of_measurement == "ft", "m", unit_of_measurement))
# Preenchimento dos missings com a média de acordo com o país
unique(coffee$country_of_origin)
coffee <- coffee %>%
group_by(country_of_origin) %>%
mutate(altitude_mean_meters = ifelse(is.na(altitude_mean_meters),
mean(altitude_mean_meters, na.rm = TRUE),
altitude_mean_meters))
# Trocando "NA" por "other"
coffee$variety_index <- match(df$variety, unique(df$variety)) # criando indice
coffee$variety <- ifelse(is.na(coffee$variety), "Other", coffee$variety)
quantidade_other <- table(coffee$variety)["Other"] #336 vezes
## não achei a forma de trocar pela mediada ... ainda buscando :(
# Criando a coluna continentes de acordo com o nome do country
coffee$continent <-
countrycode(sourcevar = coffee$country_of_origin,
origin = "country.name", destination = "continent")
view(coffee)
# Calculando a proporção de valores missing por variável
vif <- vif(lm(total_cup_points ~., data = coffee))
view(coffee)
#### TRATANDO A BASE ANTES DE APLICAR MODELOS ############
# Removendo as variáveis que não serão utilizadas
coffee <- subset (df,
select = -c(owner
,farm_name
,lot_number
,mill
,ico_number
,company
,altitude
,region
,producer
,number_of_bags
,bag_weight
,in_country_partner
,harvest_year
,grading_date
,owner_1
#,variety
,expiration
,certification_body
,certification_address
,certification_contact
,altitude_low_meters
,altitude_high_meters
,unit_of_measurement))
# Trocar missing por "Washed / Wet"
unique(coffee$processing_method)
coffee <- coffee %>%
mutate(processing_method =
ifelse(is.na(processing_method), "Washed / Wet", processing_method))
# Trocar missing por "Green"
unique(coffee$color)
coffee <- coffee %>%
mutate(color = ifelse(is.na(color), "Green", color))
# Padronizar "unit_of_measurement" em metros
#unique(coffee$unit_of_measurement)
#view(coffee$unit_of_measurement)
#coffee <- coffee %>%
#  mutate(unit_of_measurement =
#           ifelse(unit_of_measurement == "ft", "m", unit_of_measurement))
# Preenchimento dos missings com a média de acordo com o país
unique(coffee$country_of_origin)
coffee <- coffee %>%
group_by(country_of_origin) %>%
mutate(altitude_mean_meters = ifelse(is.na(altitude_mean_meters),
mean(altitude_mean_meters, na.rm = TRUE),
altitude_mean_meters))
# Trocando "NA" por "other"
coffee$variety_index <- match(df$variety, unique(df$variety)) # criando indice
coffee$variety <- ifelse(is.na(coffee$variety), "Other", coffee$variety)
quantidade_other <- table(coffee$variety)["Other"] #336 vezes
## não achei a forma de trocar pela mediada ... ainda buscando :(
# Criando a coluna continentes de acordo com o nome do country
coffee$continent <-
countrycode(sourcevar = coffee$country_of_origin,
origin = "country.name", destination = "continent")
view(coffee)
# Calculando a proporção de valores missing por variável
vif <- vif(lm(total_cup_points ~., data = coffee))
# Calculando a proporção de valores missing por variável
vif <- vif(lm(total_cup_points ~., data = coffee))
# Calculando a proporção de valores missing por variável
vif_result <- car::vif(lm(total_cup_points ~., data = coffee))
cor_matrix <- cor(coffee) # Correlation matrix
highly_correlated <- findCorrelation(cor_matrix, cutoff = 0.7)
# Calculando a proporção de valores missing por variável
vif_result <- car::vif(lm(total_cup_points ~., data = coffee))
# Removendo as variáveis que não serão utilizadas
coffee <- subset (df,
select = -c(owner
,farm_name
,lot_number
,mill
,ico_number
,company
,altitude
,region
,producer
,number_of_bags
,bag_weight
,in_country_partner
,harvest_year
,grading_date
,owner_1
#,variety
,expiration
,certification_body
,certification_address
,certification_contact
,altitude_low_meters
,altitude_high_meters
,unit_of_measurement))
# Trocar missing por "Washed / Wet"
unique(coffee$processing_method)
coffee <- coffee %>%
mutate(processing_method =
ifelse(is.na(processing_method), "Washed / Wet", processing_method))
# Trocar missing por "Green"
unique(coffee$color)
coffee <- coffee %>%
mutate(color = ifelse(is.na(color), "Green", color))
# Padronizar "unit_of_measurement" em metros
#unique(coffee$unit_of_measurement)
#view(coffee$unit_of_measurement)
#coffee <- coffee %>%
#  mutate(unit_of_measurement =
#           ifelse(unit_of_measurement == "ft", "m", unit_of_measurement))
# Preenchimento dos missings com a média de acordo com o país
unique(coffee$country_of_origin)
coffee <- coffee %>%
group_by(country_of_origin) %>%
mutate(altitude_mean_meters = ifelse(is.na(altitude_mean_meters),
mean(altitude_mean_meters, na.rm = TRUE),
altitude_mean_meters))
# Trocando "NA" por "other"
#coffee$variety_index <- match(df$variety, unique(df$variety)) # criando indice
coffee$variety <- ifelse(is.na(coffee$variety), "Other", coffee$variety)
quantidade_other <- table(coffee$variety)["Other"] #336 vezes
## não achei a forma de trocar pela mediada ... ainda buscando :(
# Criando a coluna continentes de acordo com o nome do country
coffee$continent <-
countrycode(sourcevar = coffee$country_of_origin,
origin = "country.name", destination = "continent")
view(coffee)
# Calculando a proporção de valores missing por variável
vif_result <- car::vif(lm(total_cup_points ~., data = coffee))
